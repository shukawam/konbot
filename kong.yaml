_format_version: "3.0"
_transform: true

services:
  - name: chat-service
    host: httpbin.org
    port: 80
    protocol: http
    routes:
      - name: llm-route
        paths:
          - /chat
        strip_path: true

plugins:
  ################
  # AI Gateway
  ################
  - name: ai-proxy
    service: chat-service
    config:
      route_type: llm/v1/chat
      auth:
        header_name: Authorization
        header_value: Bearer ${{ env "DECK_OPENAI_API_KEY" }}
      model:
        provider: openai
        name: gpt-4o-mini
        options:
          input_cost: 0.15
          output_cost: 0.6
          max_tokens: 1024
          temperature: 0
          top_k: 100
          top_p: 0
  # - name: ai-proxy-advanced
  #   service: chat-service
  #   config:
  #     balancer:
  #       algorithm: lowest-usage
  #       latency_strategy: tpot
  #     targets:
  #       - model:
  #           provider: openai
  #           name: gpt-4o-mini
  #           options:
  #             input_cost: 0.15
  #             output_cost: 0.6
  #             max_tokens: 1024
  #             temperature: 0
  #             top_k: 100
  #             top_p: 0
  #         route_type: llm/v1/chat
  #         auth:
  #           header_name: Authorization
  #           header_value: Bearer ${{ env "DECK_OPENAI_API_KEY" }}
  #         logging:
  #           log_payloads: true
  #           log_statistics: true
  #       - model:
  #           provider: openai
  #           name: gpt-4.1-mini
  #           options:
  #             input_cost: 0.4
  #             output_cost: 1.6
  #             max_tokens: 1024
  #             temperature: 0
  #             top_k: 100
  #             top_p: 0
  #         route_type: llm/v1/chat
  #         auth:
  #           header_name: Authorization
  #           header_value: Bearer ${{ env "DECK_OPENAI_API_KEY" }}
  #         logging:
  #           log_payloads: true
  #           log_statistics: true
  #       - model:
  #           provider: openai
  #           name: gpt-4.1-nano
  #           options:
  #             input_cost: 0.1
  #             output_cost: 0.4
  #             max_tokens: 1024
  #             temperature: 0
  #             top_k: 100
  #             top_p: 0
  #         route_type: llm/v1/chat
  #         auth:
  #           header_name: Authorization
  #           header_value: Bearer ${{ env "DECK_OPENAI_API_KEY" }}
  #         logging:
  #           log_payloads: true
  #           log_statistics: true
  - name: ai-prompt-decorator
    service: chat-service
    config:
      prompts:
        prepend:
          - role: system
            content: |
              あなたは賢いゴリラです。
              質問に対して必ずゴリラになりきって「ウホ」や「ゴリ」などの口調で回答してください。
  - name: ai-rag-injector
    service: chat-service
    config:
      embeddings:
        auth:
          header_name: Authorization
          header_value: Bearer ${{ env "DECK_OPENAI_API_KEY" }}
        model:
          provider: openai
          name: text-embedding-3-small
      vectordb:
        strategy: redis
        redis:
          host: redis
          port: 6379
        distance_metric: cosine
        dimensions: 1536
      vectordb_namespace: kong_rag_injector
  - name: ai-semantic-cache
    service: chat-service
    config:
      embeddings:
        auth:
          header_name: Authorization
          header_value: Bearer ${{ env "DECK_OPENAI_API_KEY" }}
        model:
          provider: openai
          name: text-embedding-3-small
      vectordb:
        dimensions: 1536
        distance_metric: cosine
        strategy: redis
        threshold: 0.1
        redis:
          host: redis
          port: 6379
  # - name: ai-prompt-guard
  #   service: chat-service
  #   config:
  #     allow_patterns:
  #       - ".*Kong.*"
  #     deny_patterns:
  #       - ".*コング.*"
  # - name: ai-semantic-prompt-guard
  #   service: chat-service
  #   config:
  #     embeddings:
  #       auth:
  #         header_name: Authorization
  #         header_value: Bearer ${{ env "DECK_OPENAI_API_KEY" }}
  #       model:
  #         provider: openai
  #         name: text-embedding-3-small
  #     search:
  #       threshold: 0.5
  #     vectordb:
  #       strategy: redis
  #       distance_metric: cosine
  #       threshold: 0.7
  #       dimensions: 1536
  #       redis:
  #         host: redis
  #         port: 6379
  #     rules:
  #       allow_prompts:
  #         - Kong Gatewayに関する質問
  #         - ゴリラに関する質問
  #         - ロケットに関する質問
  #       deny_prompts:
  #         - 政治に関する質問
  #         - 野球に関する質問
  # - name: ai-aws-guardrails
  #   service: chat-service
  #   config:
  #     guardrails_id: ${{ env "DECK_GUARDRAILS_ID" }}
  #     guardrails_version: '${{ env "DECK_GUARDRAILS_VERSION" }}'
  #     aws_region: ${{ env "DECK_AWS_REGION" }}
  #     aws_access_key_id: ${{ env "DECK_AWS_ACCESS_KEY_ID" }}
  #     aws_secret_access_key: ${{ env "DECK_AWS_SECRET_ACCESS_KEY" }}
  - name: ai-prompt-compressor
    config:
      compressor_type: rate
      compressor_url: http://compressor:8080
      keepalive_timeout: 60000
      log_text_data: false
      stop_on_error: true
      timeout: 10000
      compression_ranges:
        - min_tokens: 20
          max_tokens: 100
          value: 0.8
        - min_tokens: 100
          max_tokens: 1000000
          value: 0.3
  ################
  # Observability
  ################
  - name: opentelemetry
    config:
      traces_endpoint: http://otel-collector:4318/v1/traces
      logs_endpoint: http://otel-collector:4318/v1/logs
      resource_attributes:
        service.name: kong-otel-plugin
  - name: http-log
    config:
      http_endpoint: http://fluent-bit:2020
      custom_fields_by_lua:
        traceid: |
          local h = kong.request.get_header('traceparent')
          return h:match("%-([a-f0-9]+)%-[a-f0-9]+%-")
        spanid: |
          local h = kong.request.get_header('traceparent')
          return h:match("%-[a-f0-9]+%-([a-f0-9]+)%-")
  - name: prometheus
    config:
      per_consumer: true
      status_code_metrics: true
      ai_metrics: true
      latency_metrics: true
      bandwidth_metrics: true
      upstream_health_metrics: true
      wasm_metrics: true

